#!/bin/bash
#SBATCH --nodes=4            #Numero de Nós
#SBATCH --ntasks-per-node=24 #Numero de tarefas por Nó
#SBATCH --ntasks=96           #Numero total de tarefas MPI
#SBATCH -p cpu_small 	      #Fila (partition) a ser utilizada (lista: https://sdumont.lncc.br/support_manual.php?pg=support#5)
#SBATCH -J a3n-vs-a1n         #Nome job
#SBATCH --exclusive          #Utilização exclusiva dos nós durante a execução do job

#SBATCH --mail-type=END,FAIL,TIME_LIMIT
#SBATCH --mail-user=andersonrochatavares@gmail.com

#IGNORE_SBATCH --time=00:40:00

#Exibe os nós alocados para o Job
echo $SLURM_JOB_NODELIST
nodeset -e $SLURM_JOB_NODELIST

cd $SLURM_SUBMIT_DIR

echo $PWD

#Configura o script intermediario
SCRIPT=${PWD}/slurm/startjobs.sh

# carrega o java 11 e o python3
module load java/jdk-11
module load python/3.7.2

# cria as execucoes
./scripts/generate_a3n-vs-a1n.sh results/a3n-vs-a1n_sdumont_cpu-small > jobqueue/todo.txt

echo "Finished generating the job list (`wc -l jobqueue/todo.txt` jobs)"

# runs 4 times the script that will launch 24 file job clients
srun --resv-ports  --nodes 1 --ntasks=1 -c 24 $SCRIPT 1 24  & #log_run_node1 &
srun --resv-ports  --nodes 1 --ntasks=1 -c 24 $SCRIPT 25 48 & #log_run_node2 &
srun --resv-ports  --nodes 1 --ntasks=1 -c 24 $SCRIPT 49 72 & #log_run_node2 &
srun --resv-ports  --nodes 1 --ntasks=1 -c 24 $SCRIPT 73 96 & #log_run_node2 &

echo "Jobs submitted"

wait
